<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Daniel Brunner: Posts tagged 'IT'</title>
  <description>Daniel Brunner: Posts tagged 'IT'</description>
  <link>http://www.dbrunner.de/tags/IT.html</link>
  <lastBuildDate>Wed, 14 Jun 2017 10:18:02 UT</lastBuildDate>
  <pubDate>Wed, 14 Jun 2017 10:18:02 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Folgen des Lexmark-Urteils</title>
   <link>http://www.dbrunner.de/2017/06/14/folgen-des-lexmark-urteils</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2017-06-14-folgen-des-lexmark-urteils</guid>
   <pubDate>Wed, 14 Jun 2017 10:18:02 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Zum Urteil des Supreme Courts über die Patentklage von Lexmark (gebloggt &lt;a href="/2017/05/31/lexmark-scheitert-mit-einer-patentklage-in-den-usa/"&gt;hier&lt;/a&gt;) habe ich in der &lt;a href="http://www.lto.de"&gt;Legal Tribune Online&lt;/a&gt; eine interessante Einordnung &lt;a href="http://www.lto.de/recht/hintergruende/h/us-supreme-court-urteil-lexmark-drucker-patronen-patentrechte-erschoepfung-welthandel-europa/"&gt;gelesen&lt;/a&gt;. In dem Beitrag wird darauf hingewiesen, dass die Entscheidung, dass beim Export von Gütern der Patentschutz erschöpft, eine Abkehr der bisherigen Rechtsprechung in den USA sei. Dies habe gravierende Auswirkungen auf den Export patentgeschützter Güter wie Medikamente oder Lizenzverträge soweit sie sich auf die Erschöpfung von Patenten beziehen. Damit wird es für die US-Patenthinaher schwierig, den Re-Import mit dem Verweis auf das Patentrecht abzuwehren. Zugleich weist der Autor auf die Auswirkungen im internationalen Handel hin und macht deutlich, dass die Frage der Erschöpfung bei Patenten wohl eine der umstrittensten Fragen im Welthandel darstellt.&lt;/p&gt;</description></item>
  <item>
   <title>Lexmark scheitert mit einer Patentklage in den USA</title>
   <link>http://www.dbrunner.de/2017/05/31/lexmark-scheitert-mit-einer-patentklage-in-den-usa</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2017-05-31-lexmark-scheitert-mit-einer-patentklage-in-den-usa</guid>
   <pubDate>Wed, 31 May 2017 08:15:51 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Ich hatte im Jahr 2015 einmal über Lexmarks Druckerpatronen-&amp;ldquo;Lizenz&amp;rdquo; &lt;a href="/2015/02/17/lexmarks-druckerpatronen-lizenz/"&gt;gebloggt&lt;/a&gt;. Mit dieser sollte erreicht werden, dass rabattierte Patronen direkt wieder an Lexmark zurückgehen, zumindest verpflichtet sich wie gebloggt der Endkunde dazu, diese Patrone nur einmal zu nuzen und sie auch an Lexmark zurückzugeben.&lt;/p&gt;

&lt;p&gt;In den USA hat Lexmark nun versucht, das Wiederauffüllen sowie den Reimport gebrauchter Patronen durch andere Unternehmen gerichtlich zu unterbinden. Hierzu bediente sich Lexmark einer Patentklage gegen diese &amp;ldquo;Remanufacturer&amp;rdquo;. Diese waberte dann durch die Instanzen und im Ergebnis hat der Supreme Court entschieden, dass die Patentrechte nach dem Verkauf der Patronen (für die im Inland, also den USA, sowie den ins Ausland verkauften) erschöpft sind. Insbesondere stellt der Supreme Court fest, dass die Endkunden-Lizenz möglicherweise ein gültiger und durchsetzbarer Vertrag sein könnte, sich hieraus aber keine patentrechtlichen Ansprüche ableiten lassen.&lt;/p&gt;

&lt;p&gt;Man wird sehen, wie die Druckerpatronen-Industrie darauf reagieren wird.&lt;/p&gt;

&lt;p&gt;Die Entscheidung des Supreme Courts findet sich &lt;a href="https://www.supremecourt.gov/opinions/16pdf/15-1189_ebfj.pdf"&gt;hier&lt;/a&gt;. Hingewiesen wurde ich beim Lesen von &lt;a href="https://blog.fefe.de/?ts=a7d3257d"&gt;Fefes Blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update (2017&amp;ndash;06&amp;ndash;04)&lt;/em&gt; Und hier noch der Link zur Verfahrensseite beim &lt;a href="http://www.scotusblog.com/case-files/cases/impression-products-inc-v-lexmark-international-inc/"&gt;SCOTUSblog&lt;/a&gt;.&lt;/p&gt;</description></item>
  <item>
   <title>10th European Lisp Symposium</title>
   <link>http://www.dbrunner.de/2017/04/05/10th-european-lisp-symposium</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2017-04-05-10th-european-lisp-symposium</guid>
   <pubDate>Wed, 05 Apr 2017 14:00:31 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Last year my brother Stephan and I attended the European Lisp Symposium (ELS) for the first time. It was a great event and therefore we decided to come to &lt;a href="http://www.european-lisp-symposium.org/2017/index.html"&gt;this year&amp;rsquo;s symposium&lt;/a&gt;. It took place in Brussels from April 3 to April 4.&lt;/p&gt;

&lt;p&gt;After giving a two-days course on IoT using Racket at &lt;a href="http://www.studiumplus.de"&gt;StudiumPlus&lt;/a&gt; last year we submitted a short demonstration. And: It got accepted! It was a great honor for me to give a demonstration at the symposium.&lt;/p&gt;

&lt;p&gt;Our paper is in the proceedings which can be found find on the &lt;a href="http://www.european-lisp-symposium.org/2017/index.html"&gt;conference&amp;rsquo;s homepage&lt;/a&gt;. The slides are available there as well. I host these files here: &lt;a href="/pub/els2017.pdf"&gt;Paper&lt;/a&gt; and &lt;a href="/pub/els2017-slides.pdf"&gt;slides&lt;/a&gt;. The source code is provided as a &lt;a href="/pub/els2017-src.zip"&gt;ZIP archive&lt;/a&gt;.&lt;/p&gt;</description></item>
  <item>
   <title>Neues vom E-Postbrief</title>
   <link>http://www.dbrunner.de/2017/02/19/neues-vom-e-postbrief</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2017-02-19-neues-vom-e-postbrief</guid>
   <pubDate>Sun, 19 Feb 2017 14:59:09 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Im Sommer des Jahres 2010 bloggte ich (&lt;a href="/2010/07/25/der-e-postbrief-ein-selbstversuch/"&gt;hier&lt;/a&gt; und &lt;a href="/2010/07/26/nachtrag-i-zum-e-postbrief"&gt;hier&lt;/a&gt;) über meinen Selbstversuch mit dem &lt;a href="https://www.epost.de/"&gt;E-Postbrief&lt;/a&gt; (heißt wohl nun &amp;ldquo;E-POST&amp;rdquo;) der Deutschen Post. Zwischenzeitlich musste ich einmal das Kennwort erneuern. Bei dem Telefonat mit der Hotline fiel dem freundlichen Mitarbeiter auf, dass ich mich praktisch nie (es war damals über 12 Monate her) in den Dienst einloggte. Ob ich das Produkt denn wirklich noch weiter haben möchte, wollte er wissen. Nun, ich entschied mich erst einmal dabei zu bleiben. Ca. einmal im Jahr sendete mir die Post einen Newsletter in das Postfach, manchmal habe ich ihn gelesen, manchmal auch ignoriert.&lt;/p&gt;

&lt;p&gt;Jedoch, am frühen Anbend des 31. Januar 2016 schreckte ich hoch, denn eine frisch eingegangene SMS deutete darauf hin, dass noch jemand anderes außer dem Kundenservice mir etwas mitteilen wollte. Und tatsächlich, nach über sechs Jahren: Mein erster E-Postbrief!&lt;/p&gt;

&lt;p&gt;(Und was war es? Die Rechnung für ein Zeitschriften-Abonnement.)&lt;/p&gt;</description></item>
  <item>
   <title>Using Racket Minimal and raco</title>
   <link>http://www.dbrunner.de/2016/01/12/using-racket-minimal-and-raco</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2016-01-12-using-racket-minimal-and-raco</guid>
   <pubDate>Tue, 12 Jan 2016 09:55:43 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;I use Racket Minimal on my smart phone (&lt;a href="../2015/08/27/how-to-run-racket-on-the-raspberry-pi-2/"&gt;this&lt;/a&gt; describes how to compile the run time for an ARM based system). It&amp;rsquo;s is a very small installation of Racket (about 36 MB after installation). After installation one only needs to install the packages that are really neded. But this can be a bit tricky because a lot of packages want to install their documentation and other stuff and bring a whole bunch of files on your drive as dependencies.&lt;/p&gt;

&lt;p&gt;Some of the packages are divided up into a "-lib", "-doc" (and sometimes "-test") as laid out in the &lt;a href="https://docs.racket-lang.org/pkg/getting-started.html#%28part._.Naming_and_.Designing_.Packages%29"&gt;documentation&lt;/a&gt;. With these packages it&amp;rsquo;s easier to only install the implementation.&lt;/p&gt;

&lt;p&gt;A small script of mine used only basic modules and relied on &lt;code&gt;rackunit&lt;/code&gt; for the tests. On a mobile device the start up time of such a program can be  critical. Therefore it is wise to only require the needed packages and to have the source code being compiled to byte code. One could do this with &lt;code&gt;raco setup&lt;/code&gt; (which is included in Minimal Racket) but I wanted to have &lt;code&gt;raco make&lt;/code&gt; (which is not part of Minimal Racket) available.&lt;/p&gt;

&lt;p&gt;The commands of &lt;code&gt;raco&lt;/code&gt; are added via a &lt;code&gt;raco-commands&lt;/code&gt; variable in packages&amp;rsquo; &lt;code&gt;info.rkt&lt;/code&gt; file. I looked through the packages of my &amp;ldquo;full install&amp;rdquo; and found the package &lt;code&gt;compiler-lib&lt;/code&gt; which adds some commands (&lt;code&gt;make&lt;/code&gt;, &lt;code&gt;exe&lt;/code&gt;, &lt;code&gt;pack&lt;/code&gt;, &lt;code&gt;unpack&lt;/code&gt;, &lt;code&gt;decompile&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, &lt;code&gt;expand&lt;/code&gt;, &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;distribute&lt;/code&gt;, &lt;code&gt;demodularize&lt;/code&gt;) to &lt;a href="https://mirror.racket-lang.org/releases/6.3/doc/raco/index.html"&gt;&lt;code&gt;raco&lt;/code&gt;&lt;/a&gt; and relies on only a few other packages. As a result the source and binary files need about 3.8 MB on my phone which is okay for me.&lt;/p&gt;

&lt;p&gt;To sum up: After a simple &lt;code&gt;raco pkg install compiler-lib&lt;/code&gt; I could easily use &lt;code&gt;raco make&lt;/code&gt; and &lt;code&gt;raco test&lt;/code&gt; to play with my program on my phone.&lt;/p&gt;</description></item>
  <item>
   <title>I played with CHICKEN Scheme, Docker and Alpine Linux</title>
   <link>http://www.dbrunner.de/2015/12/19/i-played-with-chicken-scheme-docker-and-alpine-linux</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-12-19-i-played-with-chicken-scheme-docker-and-alpine-linux</guid>
   <pubDate>Sat, 19 Dec 2015 16:54:50 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;I am looking forward to meet LISP people at the &lt;a href="https://events.ccc.de/congress/2015/wiki/Main_Page"&gt;32c3&amp;rsquo;s&lt;/a&gt; &lt;a href="https://events.ccc.de/congress/2015/wiki/Assembly:The_%28un%29employed_schemers_%26_lispers_guild"&gt;LISP assembly&lt;/a&gt;. The last days I played a bit with different Scheme implementations including  &lt;a href="http://call-cc.org"&gt;CHICKEN scheme&lt;/a&gt;. The main feature of CHICKEN is that it compiles the Scheme code to C and then creates dynamic libraries and binaries with the C compiler. I thought that combining these binaries with a minimal Docker container could give me a very small deployment. So here are my steps:&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="choosing-alpine-linux-as-a-small-linux"&gt;Choosing Alpine Linux as a &amp;ldquo;small&amp;rdquo; Linux&lt;/h2&gt;

&lt;p&gt;The smallest Linux image for Docker is undoubtly busybox with a size of about 2.489 MB. But busybox lacks a package manager which makes installing software painful. Therefore I have chosen &lt;a href="http://alpinelinux.org"&gt;Alpine Linux&lt;/a&gt; which comes with package manager and it&amp;rsquo;s image&amp;rsquo;s size is about 5.234 MB. That&amp;rsquo;s double the size of the busybox image but still quite small compared to the Ubuntu image which is about 266 MB.&lt;/p&gt;

&lt;h2 id="creating-a-docker-container-with-chicken"&gt;Creating a Docker container with CHICKEN&lt;/h2&gt;

&lt;p&gt;Alpine Linux comes with the &lt;a href="http://www.muscl-libc.org"&gt;musl libc&lt;/a&gt; and I thought it would be best to compile all the CHICKEN stuff with that libc. Therefore I created a Docker container with gcc and all the other stuff with this Dockerfile (&lt;a href="https://github.com/krrrcks/chicken-docker-alpine"&gt;Github repository&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM alpine:3.2

RUN apk update &amp;amp;&amp;amp; apk add make gcc musl-dev 
RUN wget -O - http://code.call-cc.org/releases/4.10.0/chicken-4.10.0.tar.gz | tar xz

WORKDIR /chicken-4.10.0

RUN make PLATFORM=linux &amp;amp;&amp;amp; make PLATFORM=linux install

RUN rm -fr /chicken-4.10.0 

WORKDIR /

CMD ["csi"]&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This image is quite big (about 161.7 MB) and is available for download at the &lt;a href="https://hub.docker.com/r/krrrcks/chicken-alpine/"&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="compiling-some-chicken-code"&gt;Compiling some CHICKEN code&lt;/h2&gt;

&lt;p&gt;For testing purposes I wanted a minimal web server running in the Alpine Linux image. Therefore I looked through the &lt;a href="http://wiki.call-cc.org/chicken-projects/egg-index-4.html"&gt;egg index&lt;/a&gt; and found &lt;a href="http://wiki.call-cc.org/eggref/4/spiffy"&gt;spiffy&lt;/a&gt;. I fired up the &lt;code&gt;chicken-alpine&lt;/code&gt; container (but I used &lt;code&gt;ash&lt;/code&gt; as command instead of the &lt;code&gt;csi&lt;/code&gt; Scheme interpreter) and created a small web server that serves some static pages. I wrote a &lt;code&gt;main.scm&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(use spiffy)
(start-server)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and added some static pages to a &lt;code&gt;./web&lt;/code&gt; sub-directory. Then everything had to be compiled and prepared for deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chicken-install spiffy
csc -deploy main.scm
chicken-install -deploy -p $PWD/main spiffy&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="deploy-in-a-fresh-alpine-linux-image"&gt;Deploy in a fresh Alpine Linux image&lt;/h2&gt;

&lt;p&gt;After the compilation I copied the &lt;code&gt;main&lt;/code&gt; and &lt;code&gt;web&lt;/code&gt; directories on my host machine using &lt;code&gt;docker cp&lt;/code&gt; and created the following Dockerfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM alpine:3.2

ADD main /main
ADD web main/web
WORKDIR main

CMD /main/main&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and let &lt;code&gt;docker build -t krrrcks/spiffy-test .&lt;/code&gt; do the job. The size of the resulting image is about 12.37 MB and that&amp;rsquo;s pretty small. I uploaded that image to the &lt;a href="https://hub.docker.com/r/krrrcks/spiffy-test/"&gt;Docker Hub&lt;/a&gt; as well.&lt;/p&gt;

&lt;p&gt;To serve the pages I did a &lt;code&gt;docker run -d -p 8080:8080 krrrcks/spiffy-test&lt;/code&gt;  (spiffy listens on port 8080 in the default install) and browsed my static pages.&lt;/p&gt;</description></item>
  <item>
   <title>How to use GET Bucket location on Amazon S3 with Racket</title>
   <link>http://www.dbrunner.de/2015/09/04/how-to-use-get-bucket-location-on-amazon-s3-with-racket</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-09-04-how-to-use-get-bucket-location-on-amazon-s3-with-racket</guid>
   <pubDate>Fri, 04 Sep 2015 05:23:43 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;In &lt;a href="http://www.racket-lang.org"&gt;Racket&lt;/a&gt; I want to iterate over my buckets in Amazon S3. They are located in different regions. So how do I get my bucket&amp;rsquo;s location/region? In the API Reference there is a call &lt;a href="http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html"&gt;GET Bucket location&lt;/a&gt;. I use &lt;a href="https://github.com/greghendershott/aws"&gt;Greg&amp;rsquo;s AWS library for Racket&lt;/a&gt; and this library authenticates its calls with &lt;a href="http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html"&gt;signature version V4&lt;/a&gt;. But V4 requires the user to know the &lt;em&gt;region&lt;/em&gt; to correctly sign the request. So I need to know the region to ask Amazon S3 for the region where the bucket is located. Otherwise Amazon S3 responds with an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;
&amp;lt;Error&amp;gt;
 &amp;lt;Code&amp;gt;AuthorizationHeaderMalformed&amp;lt;/Code&amp;gt;
 &amp;lt;Message&amp;gt;The authorization header is malformed; the region 'us-east-1'
is wrong; expecting 'eu-central-1'&amp;lt;/Message&amp;gt;
 &amp;lt;Region&amp;gt;eu-central-1&amp;lt;/Region&amp;gt;
 &amp;lt;RequestId&amp;gt;XXXX&amp;lt;/RequestId&amp;gt;
 &amp;lt;HostId&amp;gt;XXXX&amp;gt;
&amp;lt;/Error&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After some search on the net I found a &lt;a href="http://stackoverflow.com/questions/27091816/retrieve-buckets-objects-without-knowing-buckets-region-with-aws-s3-rest-api"&gt;post on Stackoverflow&lt;/a&gt; that helped to solve that issue: If I use the URL format (instead of the normally used virtual host format) I could get the location of any bucket. Every region responds with a &lt;em&gt;LocationConstraint&lt;/em&gt; answer.&lt;/p&gt;

&lt;p&gt;Therefore a code snippet for Racket could be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(define (get-bucket-location bucket)
  (parameterize
      ([s3-path-requests? #t])
    (define xpr (get/proc (string-append bucket "/?location") read-entity/xexpr))
    (and (list? xpr)
         (= (length xpr) 3)
         (third xpr))))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; (get-bucket-location "my-bucket-somewhere")
"eu-central-1"&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PS: I think official Amazon S3 documentation could be a bit more verbose on the issues with GetBucketLocation and signature V4.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; Greg added a &lt;code&gt;bucket-location&lt;/code&gt; function to his great &lt;a href="http://docs.racket-lang.org/aws/S3__Storage_.html#%28def._%28%28lib._aws%2Fs3..rkt%29._bucket-location%29%29"&gt;library&lt;/a&gt;&lt;/p&gt;</description></item>
  <item>
   <title>How to run Racket on the Raspberry Pi 2</title>
   <link>http://www.dbrunner.de/2015/08/27/how-to-run-racket-on-the-raspberry-pi-2</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-08-27-how-to-run-racket-on-the-raspberry-pi-2</guid>
   <pubDate>Thu, 27 Aug 2015 11:25:45 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;I got a &lt;a href="https://www.raspberrypi.org/products/raspberry-pi-2-model-b/"&gt;Raspberry Pi 2 Model B&lt;/a&gt; to play with. I used Raspbian image as operating system. I was wondering how difficult it is to get Racket running on the Raspberry Pi. I downloaded the &lt;a href="http://mirror.racket-lang.org/installers/6.2.1/racket-6.2.1-src-builtpkgs.tgz"&gt;Unix source + built packages&lt;/a&gt; tarball from &lt;a href="http://racket-lang.org"&gt;Racket&amp;rsquo;s homepage&lt;/a&gt; because I only wanted to compile the core of Racket. After unpacking the tarball I was suprised that the instructions were quite short:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;From this directory (where the `configure' file is), run the following
commands:

  mkdir build
  cd build
  ../configure
  make
  make install&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Between &lt;code&gt;make&lt;/code&gt; and &lt;code&gt;make install&lt;/code&gt; I had to wait for about 40 minutes but then everything was fine and I could even use DrRacket on the Raspberry Pi:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/img/2015-08-27-racket-pi.png" alt="DrRacket on Raspberry Pi" /&gt;
 &lt;p class="caption"&gt;DrRacket on Raspberry Pi&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Very nice and easy to get Racket running on ARM.&lt;/p&gt;

&lt;p&gt;PS: Because the Raspberry Pi 2 Model B has an ARMv7 processor the binary runs on my Jolla smart phone as well.&lt;/p&gt;</description></item>
  <item>
   <title>Running Racket on AWS Lambda</title>
   <link>http://www.dbrunner.de/2015/08/27/running-racket-on-aws-lambda</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-08-27-running-racket-on-aws-lambda</guid>
   <pubDate>Thu, 27 Aug 2015 10:46:57 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;I started to use AWS for some projects recently. But I only use few of their services. From time to time I look into some of there services and wonder if they are useful for my tasks. I looked into &lt;a href="http://aws.amazon.com/lambda"&gt;AWS Lambda&lt;/a&gt;, "&amp;hellip; a compute service that runs your code in response to events and automatically manages the compute resources for you, making it easy to build applications that respond quickly to new information." Nowadays these &amp;ldquo;lambda functions&amp;rdquo; could be written in NodeJS or Java. When I was looking for a roadmap of the supported languages I found an interesting &lt;a href="http://blog.0x82.com/2014/11/24/aws-lambda-functions-in-go/"&gt;blog post&lt;/a&gt; by &lt;a href="https://www.twitter.com/rubenfonseca"&gt;Ruben Fonseca&lt;/a&gt;. He explaind how to run Go code on AWS Lambda.&lt;/p&gt;

&lt;p&gt;I tried the same with &lt;a href="http://racket-lang.org"&gt;Racket&lt;/a&gt; and wrote a short Racket programm &lt;code&gt;test.rkt&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#lang racket/base

(display (format "Hello from Racket, args: ~a~%" (current-command-line-arguments)))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I used &lt;code&gt;raco&lt;/code&gt; to create a binary &lt;code&gt;test&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;raco exe --orig-exe test.rkt&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I took the NodeJS wrapper from Ruben&amp;rsquo;s blog post and put it in a file &lt;code&gt;main.js&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var child_process = require('child_process');

exports.handler = function(event, context) {
  var proc = child_process.spawn('./test', [ JSON.stringify(event) ], { stdio: 'inherit' });

  proc.on('close', function(code) {
    if(code !== 0) {
      return context.done(new Error("Process exited with non-zero status code"));
    }

    context.done(null);
  });
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I put both files in a zip archive, created a new AWS Lambda function, uploaded the zip file and invoked the function:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/img/2015-08-27-racket-aws-lambda.png" alt="Invocation of AWS Lambda function" /&gt;
 &lt;p class="caption"&gt;Invocation of AWS Lambda function&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Fine!&lt;/p&gt;

&lt;p&gt;PS: Only question is: When is AWS Lambda coming to the region &lt;code&gt;eu-central-1&lt;/code&gt;, located in Frankfurt?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Upate (2016&amp;ndash;03&amp;ndash;15):&lt;/em&gt; AWS Lambda is &lt;a href="https://aws.amazon.com/de/about-aws/whats-new/2016/03/aws-lambda-available-in-eu-frankfurt/"&gt;now available&lt;/a&gt; in the EU (Frankfurt)  region!&lt;/p&gt;</description></item>
  <item>
   <title>Lexmarks Druckerpatronen-Lizenz</title>
   <link>http://www.dbrunner.de/2015/02/17/lexmarks-druckerpatronen-lizenz</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-02-17-lexmarks-druckerpatronen-lizenz</guid>
   <pubDate>Tue, 17 Feb 2015 10:28:02 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Heute benötigte ich eine Ersatzpatrone für meinen Lexmark-Drucker. An der Aufreißlasche prangen Ausrufezeichen und der Hinweis: &amp;ldquo;Attention: Updated License Terms&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Lizenzbedingungen? Für eine Druckerpatrone? Also mal ein Blick aufs Kleingedruckte:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;Bitte vor dem Öffnen lesen. Durch das Öffnen der Verpackung oder die Verwendung der mitgelieferten patentierten Kassette erklären Sie sich mit der folgenden Lizenz-Vereinbarung einverstanden. Diese patentierte Tonerkassette wird zu einem Sonderpreis verkauft und unterliegt der Patenteinschränkung, dass sie nur einmal verwendet wird. Nach ihrer erstmaligen Verwendung verpflichten Sie sich, sie zur Wiederaufbereitung und/oder zum Recylcing nur an Lexmark zurückzugeben. Die Tonerkassette funktioniert nach der Abgabe einer bestimmten Tonermenge nicht mehr. Wenn sie ersetzt werden muss, kann sie noch Resttoner enthalten. Die Kassette ist zusätzlich so konzipiert, dass die Informationen zur Kassettenkompatibilität im Druckerspeicher automatisch aktualisiert werden. Auf diese Weise kann die Verwendung gefälschter Kassetten und/oder bestimmer Drittprodukte eingeschränkt werden. Durch die Installation der beiliegenden Kassette gestatten Sie Lexmark, diese Änderungen vorzunehmen. Wenn Sie mit den vorgenannten Bedingungen nicht einverstanden sind, geben Sie die ungeöffnete Verpackung an Ihren Händler zurück. Nicht im Rahmen dieser Bestimmungen verkaufte Ersatztonerkassetten sind unter www.lexmark.com erhältlich.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Irgendwie ja auch ein bisschen putzig, wie um einen Alltagsgegenstand wie Toner so ein Bohei gemacht wird. Zwei Gedanken kommen mir da in den Sinn: 1. Es verfestigt sich mein Eindruck, dass das Patentsystem recht nahe an kaputt ist. 2. Unternehmen, die so etwas machen, sollten weniger Geld für Juristen, Patentanwälte ausgeben und das Geld eher in coole Produkte investieren.&lt;/p&gt;</description></item>
  <item>
   <title>3. Docker-Meetup in Frankfurt</title>
   <link>http://www.dbrunner.de/2015/01/18/3-docker-meetup-in-frankfurt</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-01-18-3-docker-meetup-in-frankfurt</guid>
   <pubDate>Sun, 18 Jan 2015 19:05:35 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Am 13. Januar 2015 fand in Frankfurt das bereits &lt;a href="http://www.meetup.com/Docker-Frankfurt/events/219160756/"&gt;dritte Docker-Meetup&lt;/a&gt; statt, hier einige Notizen von mir dazu.&lt;/p&gt;

&lt;h2 id="neues-zum-them-orchestrierung"&gt;Neues zum Them Orchestrierung&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://www.twitter.com/PRossbach"&gt;Peter Rossbach&lt;/a&gt; hat in einem munteren Vortrag einige Neuerungen aus dem &amp;ldquo;Docker Universum&amp;rdquo; zum Thema Orchestrierung vorgestellt. Unter anderem &lt;a href="https://github.com/docker/machine"&gt;Docker Machine&lt;/a&gt;, &lt;a href="https://github.com/docker/swarm"&gt;Docker Swarm&lt;/a&gt;, Docker Compose (ehemals &lt;a href="http://fig.sh"&gt;fig.sh&lt;/a&gt;, das wohl aufgrund von Aussprachemehrdeutigkeiten umbenannt wurde) etc. Ein sehr interessanter Überblick, insbesondere da Peter auch die ganzen Sachen immer mal angefasst und ausprobiert hat. Im Kern scheint es mir jedoch so zu sein, als wäre die Frage nach &amp;ldquo;Was nimmt man am besten, um Docker auf einer oder mehreren Maschinen im Produktivbetrieb zu nutzen?&amp;rdquo; noch recht in Bewegung. Für mich kristallisiert sich für meine Anwendungsfälle da bisher noch keine überzeugende Lösung heraus. Was ich jedoch einmal testen werde ist das fig.sh bzw. Docker Compose, da man damit eigentlich sehr schön in einem YAML-Dokument mehrere Container und ihre Abhängigkeiten darstellen kann.&lt;/p&gt;

&lt;h2 id="docker-linking"&gt;Docker Linking&lt;/h2&gt;

&lt;p&gt;&lt;a href="http://linsenraum.de"&gt;Erkan Yanar&lt;/a&gt; hat in einem Einsteigervortrag die Grundlagen von Links zwischen Containern vorgestellt. Hier scheint die Entwicklung auch noch in Bewegung zu sein, insbesondere Links über mehrere Hosts hinweg scheinen doch noch nicht so ganz einfach handzuhaben zu sein (vorgestellt wurden &lt;a href="https://github.com/SvenDowideit/dockerfiles/blob/master/ambassador/Dockerfile"&gt;Ambassador-Ansätze&lt;/a&gt; mit &lt;a href="http://www.dest-unreach.org/socat/"&gt;socat&lt;/a&gt; und anderes).&lt;/p&gt;

&lt;p&gt;Besonders erhellend fand ich den Hinweis, dass ab Docker Version 1.3 nun bei Links zwischen den Containern die &lt;code&gt;/etc/hosts&lt;/code&gt; auch nach Neustarts von gelinkten Containern immer deren richtige IP-Adresse erhält, wohingegen die Umgebungsvariablen nur die Ursprungs-IP-Adressen enthalten (also ein klares &amp;ldquo;Verlasst Euch nicht auf die Umgebungsvariablen!&amp;rdquo;).&lt;/p&gt;

&lt;h2 id="netzwerken-mit-docker"&gt;Netzwerken mit Docker&lt;/h2&gt;

&lt;p&gt;&lt;a href="https:/www.twitter.com/aschmidt75"&gt;Andreas Schmidt&lt;/a&gt; stellte eine ganze Reihe von Varianten vor, mit denen man die Container im Netzwerk auf unterschiedliche Arten und Weisen verknoten kann. Soweit ganz interessant, aber nicht meine &amp;ldquo;Liga&amp;rdquo;, wo ich mich gut auskenne.&lt;/p&gt;

&lt;h2 id="fazit"&gt;Fazit&lt;/h2&gt;

&lt;p&gt;Bei Docker in Bezug auf Orchestrierung und Container-Linken gibt es recht viel Bewegung und für mich kristallisieren sich die überzeugenden Konzepte noch nicht so richtig heraus, um damit in eine Produktivumgebung zu gehen. Im Bereich der Entwicklung und der Tests nutze ich die Container von Docker schon recht gerne, bei Produktiv-Umgebungen schreckt mich die Vielzahl an Werkzeugen und zum Teil auch die Komplexität doch noch etwas.&lt;/p&gt;

&lt;p&gt;Jedenfalls wieder ein gutes Meetup mit Ideen und Anregungen. Ich finde das schon sehr außergewöhnlich (besonders wenn man es mit anderen Branchen vergleicht), dass sich Leute zum Austauschen über Technologie treffen, Vorträge vorbereiten etc.&lt;/p&gt;

&lt;h2 id="links-zu-den-folien"&gt;Links zu den Folien&lt;/h2&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://speakerdeck.com/rossbachp/docker-meetup-frankfurt-2015-docker-orchestration"&gt;Peter Rossbach, Docker Orchestation&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://speakerdeck.com/aschmidt75/docker-networking"&gt;Andreas Schmidt, Docker Networking&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Eindrücke vom 31C3</title>
   <link>http://www.dbrunner.de/2015/01/05/eindru%CC%88cke-vom-31c3</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-01-05-eindru-CC-88cke-vom-31c3</guid>
   <pubDate>Mon, 05 Jan 2015 16:03:00 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Dieses Jahr habe ich mich einmal aufgerafft und bin das erste mal zum Congress des CCC nach Hamburg gefahren. Im Folgenden ein paar Eindrücke:&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="drumherum"&gt;Drumherum&lt;/h2&gt;

&lt;p&gt;Ich fand den &lt;a href="http://events.ccc.de/congress/2014/wiki/Main_Page"&gt;31C3&lt;/a&gt; ein tolles Ereignis: Sehr professionell und umsichtig organisiert. Überall, wo ich hinkam, war man nett und freundlich, insgesamt eine sehr willkommende Atmosphäre. Nur der Termin, der ist ja doch etwas sperrig.&lt;/p&gt;

&lt;h2 id="vorträge"&gt;Vorträge&lt;/h2&gt;

&lt;p&gt;Ich konnte einige Vorträge anhören, von denen haben mir die folgenden besonders gut gefallen (ich habe mal die Links zu den Videos und die Links zu den Einträgen im Fahrplan aufgeführt, oft gibt es im Fahrplan auch noch zugehöriges Material und weitere Hinweise):&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6264_-_de_-_saal_1_-_201412271245_-_wir_beteiligen_uns_aktiv_an_den_diskussionen_-_martin_haase_maha.html"&gt;&amp;ldquo;Wir beteiligen uns aktiv an den Dikussionen&amp;rdquo;&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6264.html"&gt;(Link im Fahrplan)&lt;/a&gt; von Martin Haase, der die &lt;a href="http://www.digitale-agenda.de/"&gt;Digitale Agenda&lt;/a&gt; der Bundesregierung aus sprachwissenschaftlicher Sicht entlarvt als das, was es ist: Heiße Luft und wenig Konkretes, schon gar nicht für den &amp;ldquo;Bürger&amp;rdquo;. Besonders spannend fand ich den Teil des &amp;ldquo;PDF befreien&amp;rdquo;, denn die Bundesregierung hat nur ein wenig konsistentes PDF bereit gestellt, dass maha erst einmal in einen Text umwandeln musste, mit dem er mit seinen Werkzeugen arbeiten konnte. In der Diskussion wurde er dazu auch noch einmal befragt und meinte, Markdown, das sei eigentlich ein ganz gutes Format.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Die beiden SS7-Vorträge von &lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6249_-_en_-_saal_1_-_201412271715_-_ss7_locate_track_manipulate_-_tobias_engel.html"&gt;Tobias Engel&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6249.html"&gt;(Fahrplan)&lt;/a&gt; und &lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6122_-_en_-_saal_1_-_201412271830_-_mobile_self-defense_-_karsten_nohl.html"&gt;Karsten Nohl&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6122.html"&gt;(Fahrplan)&lt;/a&gt;: Die Talks fand ich incl. der Live-Vorführungen sehr eindrücklich und haben mir vor Augen geführt, dass es mit der Sicherheit im Mobilfunk noch schlechter aussieht, als ich so befürchtet habe.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6369_-_en_-_saal_1_-_201412272145_-_ecchacks_-_djb_-_tanja_lange.html"&gt;ECCHacks&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6369.html"&gt;(Fahrplan)&lt;/a&gt; von djb und Tanja Lange: Ein Bekannter empfahl mir den Vortrag und meinte, ich könnte da schon was verstehen, obwohl ich mich mit diesen Ellpitischen Kurven nicht wirklich auskenne. Der Talk war didaktisch sehr gut aufbereitet und ich habe trotz der späten Stunde ein bisschen verstanden (glaube ich), worum es da eigentlich geht.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6294_-_de_-_saal_1_-_201412281815_-_vor_windows_8_wird_gewarnt_-_ruedi.html"&gt;Vor Windows 8 wird gewarnt&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6294.html"&gt;(Fahrplan)&lt;/a&gt; von ruedi: Ein kurzweiliger Vortrag über &amp;ldquo;Secure Boot&amp;rdquo; und andere Schwierigkeiten mit &amp;ldquo;Windows 8&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6258_-_en_-_saal_1_-_201412282030_-_reconstructing_narratives_-_jacob_-_laura_poitras.html"&gt;Reconstructing naraatives&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6258.html"&gt;(Fahrplan)&lt;/a&gt; von Jacob Appelbaum und Laura Poitras: Das war im voll besetzten Saal 1 ein sehr eindrücklicher Vortrag, der mit Standing Ovations endete.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6121_-_en_-_saal_2_-_201412291715_-_what_ever_happened_to_nuclear_weapons_-_michael_buker.html"&gt;What Ever Happened to Nuclear Weapons?&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6121.html"&gt;(Fahrplan)&lt;/a&gt; von Michael Büker: Diesen Vortrag fand ich vom Aufbau und der Didaktik sehr gut vorbereitet. Als wichtige Erkenntnis habe ich für mich den &lt;a href="http://de.wikipedia.org/wiki/Kernwaffenteststopp-Vertrag"&gt;Kernwaffenstopp-Vertrag (englisch Comprehensive Nuclear-Test-Ban Treaty)&lt;/a&gt; mitgenommen, einen internationalen Vertrag, der sämtliche Kernwaffentests verbietet, der aber noch nicht in Kraft getreten ist; dieser Vertrag geht weiter als der Nuclear Test Ban Treaty aus den 1960er Jahren, der Kernwaffenversuche in der Atmosphäre, im Weltraum und unter Wasser verbietet. Dennoch gibt es hierzu schon eine &lt;a href="http://www.ctbto.org/"&gt;Organisation, die Preparatory Commission for the Comprehensive Nuclear-Test-Ban Treaty Organisation&lt;/a&gt;, der man auch auf Twitter &lt;a href="http://twitter.com/ctbto_alerts"&gt;folgen kann&lt;/a&gt;. Diese &amp;ldquo;Preparatory Commission&amp;rdquo; bereitet das Inkraftreten vor und baut ein Überwachungssystem auf.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://media.ccc.de/browse/congress/2014/31c3_-_6128_-_en_-_saal_1_-_201412291830_-_thunderstrike_efi_bootkits_for_apple_macbooks_-_trammell_hudson.html"&gt;EFI bootkits for Apple MacBooks&lt;/a&gt; &lt;a href="http://events.ccc.de/congress/2014/Fahrplan/events/6128.html"&gt;(Fahrplan)&lt;/a&gt; von Trammell Hudson: Ich fand das sehr spannend vorgetragen (incl. Hexdumps etc.), wie Trammel Hudson durch &amp;ldquo;Reverse Engineering&amp;rdquo; auf ein Sicherheitsproblem bei Apple-Produktion gestoßen ist und dieses dann in einem Proof-of-Concept auch ausnutzen konnte.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Aufgrund des riesigen Angebots an Vorträgen, Workshops etc. muss ich mir in den kommenden Tagen glaube ich noch den einen oder anderen Vortrag als Video ansehen.&lt;/p&gt;

&lt;h2 id="morgengrauen-stammtisch"&gt;MorgenGrauen-Stammtisch&lt;/h2&gt;

&lt;p&gt;Etwas spontan und nicht so sonderlich koordiniert haben wir auch einen &lt;a href="http://mg.mud.de"&gt;MorgenGrauen&lt;/a&gt;-Stammtisch ausgerufen und siehe da: Drei Spieler und zwei Gäste fanden sich ein, so dass man bei einem Bier ein wenig plaudern und sich austauschen konnte.&lt;/p&gt;</description></item>
  <item>
   <title>Aus zwei mach eins</title>
   <link>http://www.dbrunner.de/2015/01/04/aus-zwei-mach-eins</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2015-01-04-aus-zwei-mach-eins</guid>
   <pubDate>Sun, 04 Jan 2015 15:16:08 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Bisher hatte ich meine Blog-Einträge auf zwei Blogs aufgeteilt, &lt;a href="http://blog.krrrcks.net"&gt;eines&lt;/a&gt; mit mehr technischen (und zum Teil englischen Texten) und dieses hier mit deutschen Texten. Ich denke, ich werde das auf dieses eine Blog hier konzentrieren. Das reduziert doch etwas den Verwaltungsaufwand. Ich habe die Texte vom nun etwas still gelegten Blog hier herüber kopiert.&lt;/p&gt;</description></item>
  <item>
   <title>Mind the storage driver for Ubuntu cloud images (on Azure)</title>
   <link>http://www.dbrunner.de/2014/07/24/mind-the-storage-driver-for-ubuntu-cloud-images-on-azure</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2014-07-24-mind-the-storage-driver-for-ubuntu-cloud-images-on-azure</guid>
   <pubDate>Thu, 24 Jul 2014 10:30:43 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;A few days ago I wanted to build Firefox OS&amp;rsquo; newest release for a friend. Because I did not wanted these GB of code, binaries etc. on my notebook I fired up an Ubuntu image on Microsoft Azure. I feared that at a certain point in the build process I may had to download everything to my local machine and therefore I installed Docker via a simple&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install docker.io&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I started the build process as laid out on Mozilla&amp;rsquo;s Developer Network. But, during downloading the source code (that&amp;rsquo;s about 12 GB of Git repositories from Mozilla and Android), I got a &amp;ldquo;no more space left on device&amp;rdquo;. That was strange: I had a 100 GB volume attached to the VM and enough space and inodes left. After some searching I asked on the IRC channel and got a good hint: &amp;ldquo;What&amp;rsquo;s your storage driver?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Well, I thought that it&amp;rsquo;s AUFS; I wanted to add &amp;ldquo;as usual&amp;rdquo; because AUFS was available on my notebook from the beginning. But a &lt;code&gt;docker.io
info&lt;/code&gt; gave me:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker.io info
Containers: 0
Images: 0
Storage Driver: devicemapper
 Pool Name: docker-8:1-131188-pool
 Data file: /var/lib/docker/devicemapper/devicemapper/data
 Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata
 Data Space Used: 291.5 Mb
 Data Space Total: 102400.0 Mb
 Metadata Space Used: 0.7 Mb
 Metadata Space Total: 2048.0 Mb
Execution Driver: native-0.1
Kernel Version: 3.13.0-29-generic
WARNING: No swap limit support&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then learned that somehow the DeviceMapper driver only allows a certain amount of diffs and I reached that amount with my build process. (Maybe it&amp;rsquo;s possible to relax that restriction but I do not know how.)&lt;/p&gt;

&lt;p&gt;I learned as well that the Ubuntu cloud image that is provided by Microsoft Azure doesn&amp;rsquo;t have AUFS support. Therefore Docker uses the DeviceMapper storage driver instead. After I installed the AUFS support I could export the images, change the storage driver and import the images again.&lt;/p&gt;

&lt;p&gt;It would be nice seeing the Docker documentation being more detailed on those storage drivers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Update 2014&amp;ndash;10&amp;ndash;23)&lt;/strong&gt; Thanks to  &lt;a href="http://blog.iron.io/2014/10/docker-in-production-what-weve-learned.html"&gt;this blog post from Iron.io&lt;/a&gt;  I found some documentation of the devicemapper storage driver. It is  located in the  &lt;a href="https://github.com/docker/docker/tree/master/daemon/graphdriver/devmapper"&gt;Repository&lt;/a&gt;.&lt;/p&gt;</description></item>
  <item>
   <title>DateTime conversion can be tricky</title>
   <link>http://www.dbrunner.de/2014/07/24/datetime-conversion-can-be-tricky</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2014-07-24-datetime-conversion-can-be-tricky</guid>
   <pubDate>Thu, 24 Jul 2014 07:41:36 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;I wrote a small Lisp application and a JavaScript client gets some data from that application. All time stamps are returned as &amp;ldquo;Lisp&amp;rdquo; time stamps, i.e. an integer with seconds where zero equals Jan 01 1900.&lt;/p&gt;

&lt;p&gt;In the JS client the time stamp is then converted to JS time stamps, i.e. millisconds where zero equals Jan 01 1970.&lt;/p&gt;

&lt;p&gt;When testing the application I noticed that sometimes the displayed date is one day behind. For example in the data base I have Jan 05 1980 but in JavaScript I get a Jan 04 1980. But some other dates worked: A time stamp Jan 05 1970 was correctly converted to Jan 05 1970.&lt;/p&gt;

&lt;p&gt;I had a look into the JavaScript code and found:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;convA = function(ts) {
  tmp = new Date(ts*1000);
  tmp.setFullYear(tmp.getFullYear() - 70);
  return tmp.getTime();
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s likely the developer thought: &amp;ldquo;Well, it&amp;rsquo;s millisecond instead of second. Therefore I multiply by 1,000. But then I am 70 years in the future and I have to substract 70 years and everything will be ok.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;After thinking a while I came to the conclusion: Of course not!&lt;/p&gt;

&lt;p&gt;The developer made the assumption that there are as many leap years between 1900 and 1970 as between &lt;code&gt;ts&lt;/code&gt; and &lt;code&gt;ts+70&lt;/code&gt;. Obviously that assumption does not hold for all time stamps. And therefore sometimes the resulting JavaScript date is one day behind.&lt;/p&gt;

&lt;p&gt;So a better solution would be to substract all seconds between 1900 and 1970 from &lt;code&gt;ts&lt;/code&gt;, multiply by 1,000 and treat this as a JavaScript time stamp. Perhaps best would be to do the conversion in the Lisp process and only deliver a JavaScript-like time stamp.&lt;/p&gt;</description></item>
  <item>
   <title>I learned something about symbols and packages</title>
   <link>http://www.dbrunner.de/2014/07/06/i-learned-something-about-symbols-and-packages</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2014-07-06-i-learned-something-about-symbols-and-packages</guid>
   <pubDate>Sun, 06 Jul 2014 07:02:39 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;I am using Common Lisp for developing a web application. Several days ago a new part of this application didn&amp;rsquo;t worked as supposed and I spent a considerable large amount of time in finding the bug. It was a very simple problem with symbols where I mixed something up.&lt;/p&gt;

&lt;p&gt;In the application the web server somewhen gets some JSON data from the browser. It is then converted to Lisp object using the &lt;code&gt;CL-JSON&lt;/code&gt; package. This package converts JSON objects to a-lists and converts the member keys to symbols (see CL-JSON&amp;rsquo;s &lt;a href="http://common-lisp.net/project/cl-json/"&gt;documentation&lt;/a&gt;. I then wanted to look something up in that a-list and failed.&lt;/p&gt;

&lt;p&gt;I wrote a small test case to show the effect and explain what went wrong.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(ql:quickload '("hunchentoot" "cl-who"))
;; direct loading via ql only for demonstration purposes, normally I
;; would use a asdf:defsystem for that.

(in-package :cl-user)

(defpackage :my-app (:use :cl))

(in-package :my-app)

(defparameter *my-a-list* 
  '((foo . 100)
    (bar . 200)))   ;; in the real application this a-list is
		    ;; generated by a JSON-to-lisp conversion by
		    ;; CL-JSON; in CL-JSON the object member keys are
		    ;; converted to symbols.

(defun get-value (key)
  "Returns the value with KEY from *MY-A-LIST*."
  (cdr (assoc (intern (string-upcase key)) *my-a-list*)))

(hunchentoot:define-easy-handler (web-get-value :uri "/get-value") (id)
  (cl-who:with-html-output-to-string (*standard-output* nil :prologue t)
    (:p (cl-who:fmt "Value of ~a is: ~a" id (get-value id)))))

(defun start ()
  (hunchentoot:start (make-instance 'hunchentoot:easy-acceptor :port 4242)))&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So on the REPL everything looks fine: &lt;code&gt;MY-APP&amp;gt; (get-value "foo")
100
MY-APP&amp;gt; (get-value "bar")
200
MY-APP&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;But when I used my web browser to give me these results as well I got something strange.  For example here are some results when using curl: &lt;code&gt;~&amp;gt; curl http://localhost:4242/get-value?id=foo
&amp;lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"&amp;gt;
&amp;lt;p&amp;gt;Value of foo is: NIL&amp;lt;/p&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I was puzzled: The value is &lt;code&gt;NIL&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;After some debugging I found out that the easy handler from Hunchentoot runs with &lt;code&gt;*package*&lt;/code&gt; set to &lt;code&gt;COMMON-LISP-USER&lt;/code&gt; (and not to &lt;code&gt;MY-APP&lt;/code&gt; as I implicitly assumed). That means that &lt;code&gt;assoc&lt;/code&gt; looked up &lt;code&gt;COMMON-LISP-USER::FOO&lt;/code&gt; in the a-list where the keys are &lt;code&gt;MY-APP::FOO&lt;/code&gt; and &lt;code&gt;MY-APP::BAR&lt;/code&gt;.  And this test fails. Therefore &lt;code&gt;NIL&lt;/code&gt; is returned which is correct.&lt;/p&gt;

&lt;p&gt;So I rewrote the &lt;code&gt;get-value&lt;/code&gt; function to: &lt;code&gt;(defun get-value (key)
  "Returns the value with KEY from *MY-A-LIST*."
  (cdr (assoc (intern (string-upcase key)
		      (find-package :my-app)) *my-a-list*)))&lt;/code&gt; Now the symbols are interned in the same package and everything went well: &lt;code&gt;~&amp;gt; curl http://localhost:4242/get-value?id=foo
&amp;lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"&amp;gt;
&amp;lt;p&amp;gt;Value of foo ist: 100&amp;lt;/p&amp;gt;

~&amp;gt; curl http://localhost:4242/get-value?id=bar
&amp;lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"&amp;gt;
&amp;lt;p&amp;gt;Value of bar ist: 200&amp;lt;/p&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Therefore I was reminded to think about packages when interning  symbols. A good guide to symbols and packages could be found in this  document: &lt;a href="http://www.flownet.com/gat/packages.pdf"&gt;The Complete Idiot&amp;rsquo;s Guide to Common Lisp Packages&lt;/a&gt;.&lt;/p&gt;</description></item>
  <item>
   <title>Unicode support for Octopress</title>
   <link>http://www.dbrunner.de/2014/06/10/unicode-support-for-octopress</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2014-06-10-unicode-support-for-octopress</guid>
   <pubDate>Tue, 10 Jun 2014 07:26:22 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Well, it seems Octopress/Jekyll would like to have a locale set for UTF&amp;ndash;8 support. I followed &lt;a href="http://www.dominik-gaetjens.de/blog/2012/06/09/utf-8-in-octopress/"&gt;this (text in German)&lt;/a&gt; hint and now my Dockerfile looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dockerfile for octopress

FROM ubuntu:14.04
MAINTAINER krrrcks &amp;lt;krrrcks@krrrcks.net&amp;gt;
ENV DEBIAN_FRONTEND noninteractive

RUN apt-get update; \
  apt-get -q -y upgrade
RUN /usr/sbin/locale-gen en_US.UTF-8; \
  update-locale LANG=en_US.UTF-8
RUN apt-get -q -y install git curl; \
  apt-get clean
RUN git clone git://github.com/imathis/octopress.git /opt/octopress
RUN curl -L https://get.rvm.io | bash -s stable --ruby
ENV HOME /root
RUN echo "export LC_ALL=en_US.UTF-8" &amp;gt;&amp;gt; /root/.bashrc
RUN echo "export LANG=en_US.UTF-8" &amp;gt;&amp;gt; /root/.bashrc
RUN echo "source /usr/local/rvm/scripts/rvm" &amp;gt;&amp;gt; /root/.bashrc; 
RUN /bin/bash -l -c "source /usr/local/rvm/scripts/rvm; \
  rvm install 1.9.3; \
  rvm use 1.9.3; \
  rvm rubygems latest; \
  cd /opt/octopress; \
  gem install bundler; \
  bundle install; \
  rake install" 
RUN echo "rvm use 1.9.3" &amp;gt;&amp;gt; /root/.bashrc

WORKDIR /opt/octopress
EXPOSE 4000
CMD ["/bin/bash"] &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After playing around with Docker and Octopress I put the whole &lt;code&gt;/opt/octopress&lt;/code&gt; folder on my host machine and then  restarted the image with the &lt;code&gt;-v&lt;/code&gt; flag. Therefore I can edit the files on my host machine with my favorite editor and use the container only for producing the HTML files, for preview and for publishing.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;rake preview&lt;/code&gt; is a neat feature because the server always looks for changed files and produces the HTML files on the fly. That means I can edit the files in my editor and could see the resulting pages in a  browser nearly the same time.&lt;/p&gt;</description></item>
  <item>
   <title>My Dockerfile for setting up Octopress</title>
   <link>http://www.dbrunner.de/2014/05/26/my-dockerfile-for-setting-up-octopress</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2014-05-26-my-dockerfile-for-setting-up-octopress</guid>
   <pubDate>Mon, 26 May 2014 07:24:00 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;After my trouble with installing all the dependencies for Octopress I  came up with the following Dockerfile for Docker. This follows the  instructions from the Octopress homepage and uses RVM for managing the ruby dependencies.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dockerfile for octopress

FROM ubuntu:14.04
MAINTAINER krrrcks &amp;lt;krrrcks@krrrcks.net&amp;gt;
ENV DEBIAN_FRONTEND noninteractive

RUN apt-get update; \
  apt-get -q -y upgrade
RUN apt-get -q -y install git curl; \
  apt-get clean
RUN git clone git://github.com/imathis/octopress.git /opt/octopress
RUN curl -L https://get.rvm.io | bash -s stable --ruby
ENV HOME /root
RUN echo "source /usr/local/rvm/scripts/rvm" &amp;gt;&amp;gt; /root/.bashrc; 
RUN /bin/bash -l -c "source /usr/local/rvm/scripts/rvm; \
  rvm install 1.9.3; \
  rvm use 1.9.3; \
  rvm rubygems latest; \
  cd /opt/octopress; \
  gem install bundler; \
  bundle install; \
  rake install" 
RUN echo "rvm use 1.9.3" &amp;gt;&amp;gt; /root/.bashrc

WORKDIR /opt/octopress
EXPOSE 4000
CMD ["/bin/bash"] &lt;/code&gt;&lt;/pre&gt;</description></item>
  <item>
   <title>Programm-Bibliotheken und verschiedene Versionsstände</title>
   <link>http://www.dbrunner.de/2011/10/16/programm-bibliotheken-und-verschiedene-versionssta%CC%88nde</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2011-10-16-programm-bibliotheken-und-verschiedene-versionssta-CC-88nde</guid>
   <pubDate>Sat, 15 Oct 2011 22:00:00 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Ich bin ein sehr intensiver Nutzer von &lt;a href="http://www.orgmode.org"&gt;Org-Mode&lt;/a&gt;, einem speziellen Modul/Modus für den Emacs-Editor (ich habe hier auf meiner Homepage auch einmal mit einer kleinen &lt;a href="http://archive.dbrunner.de/it/org-mode.html"&gt;Seite&lt;/a&gt; dazu begonnen). Nun ja, was soll man sagen: Bei Emacs wird eine recht alte Version mitgeliefert. Ich wollte dann mal auch wegen einiger Funktionen die neueste nutzen. Also via &lt;code&gt;git&lt;/code&gt; heruntergeladen und eingebunden und Peng, irgendwas funktioniert natürlich nicht mehr: Ich benötige pratisch täglich den Export von Org-Mode-Dateien nach &lt;code&gt;LaTeX&lt;/code&gt;, der wollte aber nicht mehr. Nun kann ich zwar Lisp, aber das Emacs Lisp ist doch etwas speziell und die Debug-Möglichkeiten sind etwas … nun ja, altbacken. Kurzum: Das macht überhaupt keinen Spaß. Im Endeffekt habe ich dann herausgefunden, dass unter bestimmten Umständen Teile des alten, bei Emacs mitgelieferten Codes nachgeladen wird anstelle der neuen Distribution. Diese bescheidene Art von Emacs mit Modulen und Paketen umzugehen ist wirklich erschreckend. Was dafür wiederum total toll war: In der von mir verwendeten Org-Mode-Version war wohl ein Fehler, das ganze Ding über die Mailingliste geschickt und innert einem halben Tag war der Fehler von anderen Nutzern und von Carsten Dominik auch gleich gefixt. Grandiose Antwortzeit!&lt;/p&gt;</description></item>
  <item>
   <title>Constanze Kurz/Frank Rieger, Die Datenfresser</title>
   <link>http://www.dbrunner.de/2011/06/30/constanze-kurz-frank-rieger-die-datenfresser</link>
   <guid isPermaLink="false">urn:http-www-dbrunner-de:-2011-06-30-constanze-kurz-frank-rieger-die-datenfresser</guid>
   <pubDate>Thu, 30 Jun 2011 11:00:00 UT</pubDate>
   <author>Daniel Brunner</author>
   <description>
&lt;p&gt;Nun, ich habe das Buch von Constanze Kurz und Frank Rieger, Die Datenfresser, gelesen. Hier eine sehr kurze Besprechung:&lt;/p&gt;

&lt;p&gt;Die beiden Autoren wollen erklären, &amp;ldquo;wie Internetfirmen und Staat sich unsere persönlichen Daten einverbleiben und wie wir die Kontrolle darüber zurückerlangen.&amp;rdquo; Thematisch werden automatisierte Datenanalysen, der Wert der Nutzerdaten, die ökonomischen Mechanismen hinter sozialen Netzwerken und den Daten der Nutzer und einige Aspekte des staatlichen Einsatzes zur Datenanalyse vorgestellt und diskutiert.&lt;/p&gt;

&lt;p&gt;Als wichtige Botschaft wird dem Leser mitgegeben, dass seine Daten für Unternehmen einen Wert darstellen und dass Angebote im Netz dazu dienen, diese Daten zu erlangen und zu monetarisieren; also Vorsicht vor Kostenlos-Angeboten, denn im Kern zahlt man auch für ein kostenloses Angebot, im Zweifel mit seinen Daten. Eine andere Kernbotschaft läuft darauf hinaus, dass Daten auch missbraucht werden können, sei es von staatlicher Stelle oder von zwielichtigen Zeitgenossen.&lt;/p&gt;

&lt;p&gt;Das Bändchen mit 272 Seiten ist kurzweilig geschrieben, wechselt zwischen tatsächlichen Begebenheiten, Sachbuchdarstellung, fiktiven Geschichten und netzpolitischen und gesellschaftspolitischen Erwägungen geschickt hin und her. Der leichte und nicht in technische Details verliebte Sprachstil sowie die Art, wie technische Probleme dargestellt werden, macht es sicherlich auch einem Laien einfach, den Themen zu folgen und etwas hinter die Geschäftspraktiken und die Techniken zu sehen.&lt;/p&gt;

&lt;p&gt;Das Buch leidet an mancher Stelle etwas, da die Kraft des Arguments nicht so richtig einschlagen mag. Beispiele sind insbesondere die Frage, wie man sich denn nun gegen die Datenfresser wehrt oder die Argumentation gegen die &amp;ldquo;Wer nichts zu verbergen hat…&amp;rdquo;-Ideologie. Das wirkt an der einen oder anderen Stelle zwar bemüht, aber nicht so kraftvoll, wie man sich das erhofft hat. Insbesondere ein schlüssiges Gegenkonzept hätte noch etwas breiter und prominenter ausgearbeitet werden können.&lt;/p&gt;

&lt;p&gt;Auch werden zum Teil Gefahren und Befürchtungen recht abstrakt beschrieben, manchmal auch mit Hinweis, so etwas sei schon vorgekommen, aber man hätte es dann an der einen oder anderen Stelle doch gerne genauer gewusst.&lt;/p&gt;

&lt;p&gt;Eher ein Buch zum Verschenken an Leute, die noch nicht so firm im Netz sind.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.fischerverlage.de/buch/Die_Datenfresser/9783100485182"&gt;Constanze Kurz/Frank Rieger, Die Datenfresser, 272 Seiten, S. Fischer Verlag, 16,95 EUR.&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>